[0;36m(APIServer pid=28558)[0;0m INFO 02-12 18:49:33 [utils.py:325] 
[0;36m(APIServer pid=28558)[0;0m INFO 02-12 18:49:33 [utils.py:325]        â–ˆ     â–ˆ     â–ˆâ–„   â–„â–ˆ
[0;36m(APIServer pid=28558)[0;0m INFO 02-12 18:49:33 [utils.py:325]  â–„â–„ â–„â–ˆ â–ˆ     â–ˆ     â–ˆ â–€â–„â–€ â–ˆ  version 0.15.1
[0;36m(APIServer pid=28558)[0;0m INFO 02-12 18:49:33 [utils.py:325]   â–ˆâ–„â–ˆâ–€ â–ˆ     â–ˆ     â–ˆ     â–ˆ  model   nvidia/NVIDIA-Nemotron-3-Nano-30B-A3B-BF16
[0;36m(APIServer pid=28558)[0;0m INFO 02-12 18:49:33 [utils.py:325]    â–€â–€  â–€â–€â–€â–€â–€ â–€â–€â–€â–€â–€ â–€     â–€
[0;36m(APIServer pid=28558)[0;0m INFO 02-12 18:49:33 [utils.py:325] 
[0;36m(APIServer pid=28558)[0;0m INFO 02-12 18:49:33 [utils.py:261] non-default args: {'model_tag': 'nvidia/NVIDIA-Nemotron-3-Nano-30B-A3B-BF16', 'api_server_count': 1, 'model': 'nvidia/NVIDIA-Nemotron-3-Nano-30B-A3B-BF16', 'trust_remote_code': True, 'max_model_len': 16384}
[0;36m(APIServer pid=28558)[0;0m The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.
[0;36m(APIServer pid=28558)[0;0m The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.
[0;36m(APIServer pid=28558)[0;0m INFO 02-12 18:49:33 [model.py:541] Resolved architecture: NemotronHForCausalLM
[0;36m(APIServer pid=28558)[0;0m INFO 02-12 18:49:33 [model.py:1561] Using max model len 16384
[0;36m(APIServer pid=28558)[0;0m INFO 02-12 18:49:33 [scheduler.py:226] Chunked prefill is enabled with max_num_batched_tokens=2048.
[0;36m(APIServer pid=28558)[0;0m INFO 02-12 18:49:33 [config.py:578] Updating mamba_ssm_cache_dtype to 'float32' for NemotronH model
[0;36m(APIServer pid=28558)[0;0m INFO 02-12 18:49:34 [config.py:504] Setting attention block size to 2096 tokens to ensure that attention page size is >= mamba page size.
[0;36m(APIServer pid=28558)[0;0m INFO 02-12 18:49:34 [config.py:535] Padding mamba page size by 0.58% to ensure that mamba page size and attention page size are exactly equal.
[0;36m(APIServer pid=28558)[0;0m INFO 02-12 18:49:34 [vllm.py:624] Asynchronous scheduling is enabled.
[0;36m(EngineCore_DP0 pid=28784)[0;0m INFO 02-12 18:49:41 [core.py:96] Initializing a V1 LLM engine (v0.15.1) with config: model='nvidia/NVIDIA-Nemotron-3-Nano-30B-A3B-BF16', speculative_config=None, tokenizer='nvidia/NVIDIA-Nemotron-3-Nano-30B-A3B-BF16', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=16384, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, data_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, enable_return_routed_experts=False, kv_cache_dtype=auto, device_config=cuda, structured_outputs_config=StructuredOutputsConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_parser='', reasoning_parser_plugin='', enable_in_reasoning=False), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None, kv_cache_metrics=False, kv_cache_metrics_sample=0.01, cudagraph_metrics=False, enable_layerwise_nvtx_tracing=False, enable_mfu_metrics=False, enable_mm_processor_stats=False, enable_logging_iteration_details=False), seed=0, served_model_name=nvidia/NVIDIA-Nemotron-3-Nano-30B-A3B-BF16, enable_prefix_caching=False, enable_chunked_prefill=True, pooler_config=None, compilation_config={'level': None, 'mode': <CompilationMode.VLLM_COMPILE: 3>, 'debug_dump_path': None, 'cache_dir': '', 'compile_cache_save_format': 'binary', 'backend': 'inductor', 'custom_ops': ['none'], 'splitting_ops': ['vllm::unified_attention', 'vllm::unified_attention_with_output', 'vllm::unified_mla_attention', 'vllm::unified_mla_attention_with_output', 'vllm::mamba_mixer2', 'vllm::mamba_mixer', 'vllm::short_conv', 'vllm::linear_attention', 'vllm::plamo2_mamba_mixer', 'vllm::gdn_attention_core', 'vllm::kda_attention', 'vllm::sparse_attn_indexer', 'vllm::rocm_aiter_sparse_attn_indexer', 'vllm::unified_kv_cache_update'], 'compile_mm_encoder': False, 'compile_sizes': [], 'compile_ranges_split_points': [2048], 'inductor_compile_config': {'enable_auto_functionalized_v2': False, 'combo_kernels': True, 'benchmark_combo_kernel': True}, 'inductor_passes': {}, 'cudagraph_mode': <CUDAGraphMode.FULL_AND_PIECEWISE: (2, 1)>, 'cudagraph_num_of_warmups': 1, 'cudagraph_capture_sizes': [1, 2, 4, 8, 16, 24, 32, 40, 48, 56, 64, 72, 80, 88, 96, 104, 112, 120, 128, 136, 144, 152, 160, 168, 176, 184, 192, 200, 208, 216, 224, 232, 240, 248, 256, 272, 288, 304, 320, 336, 352, 368, 384, 400, 416, 432, 448, 464, 480, 496, 512], 'cudagraph_copy_inputs': False, 'cudagraph_specialize_lora': True, 'use_inductor_graph_partition': False, 'pass_config': {'fuse_norm_quant': False, 'fuse_act_quant': False, 'fuse_attn_quant': False, 'eliminate_noops': True, 'enable_sp': False, 'fuse_gemm_comms': False, 'fuse_allreduce_rms': False}, 'max_cudagraph_capture_size': 512, 'dynamic_shapes_config': {'type': <DynamicShapesType.BACKED: 'backed'>, 'evaluate_guards': False, 'assume_32_bit_indexing': True}, 'local_cache_dir': None, 'static_all_moe_layers': []}
[0;36m(EngineCore_DP0 pid=28784)[0;0m INFO 02-12 18:49:42 [parallel_state.py:1212] world_size=1 rank=0 local_rank=0 distributed_init_method=tcp://172.20.0.2:47295 backend=nccl
[0;36m(EngineCore_DP0 pid=28784)[0;0m INFO 02-12 18:49:42 [parallel_state.py:1423] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, PCP rank 0, TP rank 0, EP rank 0
[0;36m(EngineCore_DP0 pid=28784)[0;0m INFO 02-12 18:49:42 [gpu_model_runner.py:4033] Starting to load model nvidia/NVIDIA-Nemotron-3-Nano-30B-A3B-BF16...
[0;36m(EngineCore_DP0 pid=28784)[0;0m INFO 02-12 18:49:43 [unquantized.py:103] Using TRITON backend for Unquantized MoE
[0;36m(EngineCore_DP0 pid=28784)[0;0m INFO 02-12 18:49:43 [cuda.py:364] Using FLASH_ATTN attention backend out of potential backends: ('FLASH_ATTN', 'FLASHINFER', 'TRITON_ATTN', 'FLEX_ATTENTION')
[0;36m(EngineCore_DP0 pid=28784)[0;0m ERROR 02-12 18:49:44 [gpu_model_runner.py:4128] Failed to load model - not enough GPU memory. Try lowering --gpu-memory-utilization to free memory for weights, increasing --tensor-parallel-size, or using --quantization. See https://docs.vllm.ai/en/latest/configuration/conserving_memory/ for more tips. (original error: CUDA out of memory. Tried to allocate 1.19 GiB. GPU 0 has a total capacity of 44.40 GiB of which 492.12 MiB is free. Process 3366432 has 43.91 GiB memory in use. Of the allocated memory 43.38 GiB is allocated by PyTorch, and 38.94 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables))
[0;36m(EngineCore_DP0 pid=28784)[0;0m ERROR 02-12 18:49:44 [core.py:946] EngineCore failed to start.
[0;36m(EngineCore_DP0 pid=28784)[0;0m ERROR 02-12 18:49:44 [core.py:946] Traceback (most recent call last):
[0;36m(EngineCore_DP0 pid=28784)[0;0m ERROR 02-12 18:49:44 [core.py:946]   File "/usr/local/lib/python3.11/dist-packages/vllm/v1/engine/core.py", line 937, in run_engine_core
[0;36m(EngineCore_DP0 pid=28784)[0;0m ERROR 02-12 18:49:44 [core.py:946]     engine_core = EngineCoreProc(*args, engine_index=dp_rank, **kwargs)
[0;36m(EngineCore_DP0 pid=28784)[0;0m ERROR 02-12 18:49:44 [core.py:946]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=28784)[0;0m ERROR 02-12 18:49:44 [core.py:946]   File "/usr/local/lib/python3.11/dist-packages/vllm/v1/engine/core.py", line 691, in __init__
[0;36m(EngineCore_DP0 pid=28784)[0;0m ERROR 02-12 18:49:44 [core.py:946]     super().__init__(
[0;36m(EngineCore_DP0 pid=28784)[0;0m ERROR 02-12 18:49:44 [core.py:946]   File "/usr/local/lib/python3.11/dist-packages/vllm/v1/engine/core.py", line 105, in __init__
[0;36m(EngineCore_DP0 pid=28784)[0;0m ERROR 02-12 18:49:44 [core.py:946]     self.model_executor = executor_class(vllm_config)
[0;36m(EngineCore_DP0 pid=28784)[0;0m ERROR 02-12 18:49:44 [core.py:946]                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=28784)[0;0m ERROR 02-12 18:49:44 [core.py:946]   File "/usr/local/lib/python3.11/dist-packages/vllm/v1/executor/abstract.py", line 101, in __init__
[0;36m(EngineCore_DP0 pid=28784)[0;0m ERROR 02-12 18:49:44 [core.py:946]     self._init_executor()
[0;36m(EngineCore_DP0 pid=28784)[0;0m ERROR 02-12 18:49:44 [core.py:946]   File "/usr/local/lib/python3.11/dist-packages/vllm/v1/executor/uniproc_executor.py", line 48, in _init_executor
[0;36m(EngineCore_DP0 pid=28784)[0;0m ERROR 02-12 18:49:44 [core.py:946]     self.driver_worker.load_model()
[0;36m(EngineCore_DP0 pid=28784)[0;0m ERROR 02-12 18:49:44 [core.py:946]   File "/usr/local/lib/python3.11/dist-packages/vllm/v1/worker/gpu_worker.py", line 275, in load_model
[0;36m(EngineCore_DP0 pid=28784)[0;0m ERROR 02-12 18:49:44 [core.py:946]     self.model_runner.load_model(eep_scale_up=eep_scale_up)
[0;36m(EngineCore_DP0 pid=28784)[0;0m ERROR 02-12 18:49:44 [core.py:946]   File "/usr/local/lib/python3.11/dist-packages/vllm/v1/worker/gpu_model_runner.py", line 4129, in load_model
[0;36m(EngineCore_DP0 pid=28784)[0;0m ERROR 02-12 18:49:44 [core.py:946]     raise e
[0;36m(EngineCore_DP0 pid=28784)[0;0m ERROR 02-12 18:49:44 [core.py:946]   File "/usr/local/lib/python3.11/dist-packages/vllm/v1/worker/gpu_model_runner.py", line 4052, in load_model
[0;36m(EngineCore_DP0 pid=28784)[0;0m ERROR 02-12 18:49:44 [core.py:946]     self.model = model_loader.load_model(
[0;36m(EngineCore_DP0 pid=28784)[0;0m ERROR 02-12 18:49:44 [core.py:946]                  ^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=28784)[0;0m ERROR 02-12 18:49:44 [core.py:946]   File "/usr/local/lib/python3.11/dist-packages/vllm/model_executor/model_loader/base_loader.py", line 50, in load_model
[0;36m(EngineCore_DP0 pid=28784)[0;0m ERROR 02-12 18:49:44 [core.py:946]     model = initialize_model(
[0;36m(EngineCore_DP0 pid=28784)[0;0m ERROR 02-12 18:49:44 [core.py:946]             ^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=28784)[0;0m ERROR 02-12 18:49:44 [core.py:946]   File "/usr/local/lib/python3.11/dist-packages/vllm/model_executor/model_loader/utils.py", line 48, in initialize_model
[0;36m(EngineCore_DP0 pid=28784)[0;0m ERROR 02-12 18:49:44 [core.py:946]     return model_class(vllm_config=vllm_config, prefix=prefix)
[0;36m(EngineCore_DP0 pid=28784)[0;0m ERROR 02-12 18:49:44 [core.py:946]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=28784)[0;0m ERROR 02-12 18:49:44 [core.py:946]   File "/usr/local/lib/python3.11/dist-packages/vllm/model_executor/models/nemotron_h.py", line 830, in __init__
[0;36m(EngineCore_DP0 pid=28784)[0;0m ERROR 02-12 18:49:44 [core.py:946]     self.model = NemotronHModel(
[0;36m(EngineCore_DP0 pid=28784)[0;0m ERROR 02-12 18:49:44 [core.py:946]                  ^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=28784)[0;0m ERROR 02-12 18:49:44 [core.py:946]   File "/usr/local/lib/python3.11/dist-packages/vllm/compilation/decorators.py", line 306, in __init__
[0;36m(EngineCore_DP0 pid=28784)[0;0m ERROR 02-12 18:49:44 [core.py:946]     old_init(self, **kwargs)
[0;36m(EngineCore_DP0 pid=28784)[0;0m ERROR 02-12 18:49:44 [core.py:946]   File "/usr/local/lib/python3.11/dist-packages/vllm/model_executor/models/nemotron_h.py", line 590, in __init__
[0;36m(EngineCore_DP0 pid=28784)[0;0m ERROR 02-12 18:49:44 [core.py:946]     self.start_layer, self.end_layer, self.layers = make_layers(
[0;36m(EngineCore_DP0 pid=28784)[0;0m ERROR 02-12 18:49:44 [core.py:946]                                                     ^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=28784)[0;0m ERROR 02-12 18:49:44 [core.py:946]   File "/usr/local/lib/python3.11/dist-packages/vllm/model_executor/models/utils.py", line 706, in make_layers
[0;36m(EngineCore_DP0 pid=28784)[0;0m ERROR 02-12 18:49:44 [core.py:946]     + [
[0;36m(EngineCore_DP0 pid=28784)[0;0m ERROR 02-12 18:49:44 [core.py:946]       ^
[0;36m(EngineCore_DP0 pid=28784)[0;0m ERROR 02-12 18:49:44 [core.py:946]   File "/usr/local/lib/python3.11/dist-packages/vllm/model_executor/models/utils.py", line 707, in <listcomp>
[0;36m(EngineCore_DP0 pid=28784)[0;0m ERROR 02-12 18:49:44 [core.py:946]     maybe_offload_to_cpu(layer_fn(prefix=f"{prefix}.{idx}"))
[0;36m(EngineCore_DP0 pid=28784)[0;0m ERROR 02-12 18:49:44 [core.py:946]                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=28784)[0;0m ERROR 02-12 18:49:44 [core.py:946]   File "/usr/local/lib/python3.11/dist-packages/vllm/model_executor/models/nemotron_h.py", line 580, in get_layer
[0;36m(EngineCore_DP0 pid=28784)[0;0m ERROR 02-12 18:49:44 [core.py:946]     return layer_class(
[0;36m(EngineCore_DP0 pid=28784)[0;0m ERROR 02-12 18:49:44 [core.py:946]            ^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=28784)[0;0m ERROR 02-12 18:49:44 [core.py:946]   File "/usr/local/lib/python3.11/dist-packages/vllm/model_executor/models/nemotron_h.py", line 357, in __init__
[0;36m(EngineCore_DP0 pid=28784)[0;0m ERROR 02-12 18:49:44 [core.py:946]     self.mixer = NemotronHMoE(
[0;36m(EngineCore_DP0 pid=28784)[0;0m ERROR 02-12 18:49:44 [core.py:946]                  ^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=28784)[0;0m ERROR 02-12 18:49:44 [core.py:946]   File "/usr/local/lib/python3.11/dist-packages/vllm/model_executor/models/nemotron_h.py", line 190, in __init__
[0;36m(EngineCore_DP0 pid=28784)[0;0m ERROR 02-12 18:49:44 [core.py:946]     self.experts = SharedFusedMoE(
[0;36m(EngineCore_DP0 pid=28784)[0;0m ERROR 02-12 18:49:44 [core.py:946]                    ^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=28784)[0;0m ERROR 02-12 18:49:44 [core.py:946]   File "/usr/local/lib/python3.11/dist-packages/vllm/model_executor/layers/fused_moe/shared_fused_moe.py", line 28, in __init__
[0;36m(EngineCore_DP0 pid=28784)[0;0m ERROR 02-12 18:49:44 [core.py:946]     super().__init__(**kwargs)
[0;36m(EngineCore_DP0 pid=28784)[0;0m ERROR 02-12 18:49:44 [core.py:946]   File "/usr/local/lib/python3.11/dist-packages/vllm/model_executor/layers/fused_moe/layer.py", line 639, in __init__
[0;36m(EngineCore_DP0 pid=28784)[0;0m ERROR 02-12 18:49:44 [core.py:946]     self.quant_method.create_weights(layer=self, **moe_quant_params)
[0;36m(EngineCore_DP0 pid=28784)[0;0m ERROR 02-12 18:49:44 [core.py:946]   File "/usr/local/lib/python3.11/dist-packages/vllm/model_executor/layers/fused_moe/unquantized_fused_moe_method.py", line 129, in create_weights
[0;36m(EngineCore_DP0 pid=28784)[0;0m ERROR 02-12 18:49:44 [core.py:946]     torch.empty(
[0;36m(EngineCore_DP0 pid=28784)[0;0m ERROR 02-12 18:49:44 [core.py:946]   File "/usr/local/lib/python3.11/dist-packages/torch/utils/_device.py", line 103, in __torch_function__
[0;36m(EngineCore_DP0 pid=28784)[0;0m ERROR 02-12 18:49:44 [core.py:946]     return func(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=28784)[0;0m ERROR 02-12 18:49:44 [core.py:946]            ^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=28784)[0;0m ERROR 02-12 18:49:44 [core.py:946] torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 1.19 GiB. GPU 0 has a total capacity of 44.40 GiB of which 492.12 MiB is free. Process 3366432 has 43.91 GiB memory in use. Of the allocated memory 43.38 GiB is allocated by PyTorch, and 38.94 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[0;36m(EngineCore_DP0 pid=28784)[0;0m Process EngineCore_DP0:
[0;36m(EngineCore_DP0 pid=28784)[0;0m Traceback (most recent call last):
[0;36m(EngineCore_DP0 pid=28784)[0;0m   File "/usr/lib/python3.11/multiprocessing/process.py", line 314, in _bootstrap
[0;36m(EngineCore_DP0 pid=28784)[0;0m     self.run()
[0;36m(EngineCore_DP0 pid=28784)[0;0m   File "/usr/lib/python3.11/multiprocessing/process.py", line 108, in run
[0;36m(EngineCore_DP0 pid=28784)[0;0m     self._target(*self._args, **self._kwargs)
[0;36m(EngineCore_DP0 pid=28784)[0;0m   File "/usr/local/lib/python3.11/dist-packages/vllm/v1/engine/core.py", line 950, in run_engine_core
[0;36m(EngineCore_DP0 pid=28784)[0;0m     raise e
[0;36m(EngineCore_DP0 pid=28784)[0;0m   File "/usr/local/lib/python3.11/dist-packages/vllm/v1/engine/core.py", line 937, in run_engine_core
[0;36m(EngineCore_DP0 pid=28784)[0;0m     engine_core = EngineCoreProc(*args, engine_index=dp_rank, **kwargs)
[0;36m(EngineCore_DP0 pid=28784)[0;0m                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=28784)[0;0m   File "/usr/local/lib/python3.11/dist-packages/vllm/v1/engine/core.py", line 691, in __init__
[0;36m(EngineCore_DP0 pid=28784)[0;0m     super().__init__(
[0;36m(EngineCore_DP0 pid=28784)[0;0m   File "/usr/local/lib/python3.11/dist-packages/vllm/v1/engine/core.py", line 105, in __init__
[0;36m(EngineCore_DP0 pid=28784)[0;0m     self.model_executor = executor_class(vllm_config)
[0;36m(EngineCore_DP0 pid=28784)[0;0m                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=28784)[0;0m   File "/usr/local/lib/python3.11/dist-packages/vllm/v1/executor/abstract.py", line 101, in __init__
[0;36m(EngineCore_DP0 pid=28784)[0;0m     self._init_executor()
[0;36m(EngineCore_DP0 pid=28784)[0;0m   File "/usr/local/lib/python3.11/dist-packages/vllm/v1/executor/uniproc_executor.py", line 48, in _init_executor
[0;36m(EngineCore_DP0 pid=28784)[0;0m     self.driver_worker.load_model()
[0;36m(EngineCore_DP0 pid=28784)[0;0m   File "/usr/local/lib/python3.11/dist-packages/vllm/v1/worker/gpu_worker.py", line 275, in load_model
[0;36m(EngineCore_DP0 pid=28784)[0;0m     self.model_runner.load_model(eep_scale_up=eep_scale_up)
[0;36m(EngineCore_DP0 pid=28784)[0;0m   File "/usr/local/lib/python3.11/dist-packages/vllm/v1/worker/gpu_model_runner.py", line 4129, in load_model
[0;36m(EngineCore_DP0 pid=28784)[0;0m     raise e
[0;36m(EngineCore_DP0 pid=28784)[0;0m   File "/usr/local/lib/python3.11/dist-packages/vllm/v1/worker/gpu_model_runner.py", line 4052, in load_model
[0;36m(EngineCore_DP0 pid=28784)[0;0m     self.model = model_loader.load_model(
[0;36m(EngineCore_DP0 pid=28784)[0;0m                  ^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=28784)[0;0m   File "/usr/local/lib/python3.11/dist-packages/vllm/model_executor/model_loader/base_loader.py", line 50, in load_model
[0;36m(EngineCore_DP0 pid=28784)[0;0m     model = initialize_model(
[0;36m(EngineCore_DP0 pid=28784)[0;0m             ^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=28784)[0;0m   File "/usr/local/lib/python3.11/dist-packages/vllm/model_executor/model_loader/utils.py", line 48, in initialize_model
[0;36m(EngineCore_DP0 pid=28784)[0;0m     return model_class(vllm_config=vllm_config, prefix=prefix)
[0;36m(EngineCore_DP0 pid=28784)[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=28784)[0;0m   File "/usr/local/lib/python3.11/dist-packages/vllm/model_executor/models/nemotron_h.py", line 830, in __init__
[0;36m(EngineCore_DP0 pid=28784)[0;0m     self.model = NemotronHModel(
[0;36m(EngineCore_DP0 pid=28784)[0;0m                  ^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=28784)[0;0m   File "/usr/local/lib/python3.11/dist-packages/vllm/compilation/decorators.py", line 306, in __init__
[0;36m(EngineCore_DP0 pid=28784)[0;0m     old_init(self, **kwargs)
[0;36m(EngineCore_DP0 pid=28784)[0;0m   File "/usr/local/lib/python3.11/dist-packages/vllm/model_executor/models/nemotron_h.py", line 590, in __init__
[0;36m(EngineCore_DP0 pid=28784)[0;0m     self.start_layer, self.end_layer, self.layers = make_layers(
[0;36m(EngineCore_DP0 pid=28784)[0;0m                                                     ^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=28784)[0;0m   File "/usr/local/lib/python3.11/dist-packages/vllm/model_executor/models/utils.py", line 706, in make_layers
[0;36m(EngineCore_DP0 pid=28784)[0;0m     + [
[0;36m(EngineCore_DP0 pid=28784)[0;0m       ^
[0;36m(EngineCore_DP0 pid=28784)[0;0m   File "/usr/local/lib/python3.11/dist-packages/vllm/model_executor/models/utils.py", line 707, in <listcomp>
[0;36m(EngineCore_DP0 pid=28784)[0;0m     maybe_offload_to_cpu(layer_fn(prefix=f"{prefix}.{idx}"))
[0;36m(EngineCore_DP0 pid=28784)[0;0m                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=28784)[0;0m   File "/usr/local/lib/python3.11/dist-packages/vllm/model_executor/models/nemotron_h.py", line 580, in get_layer
[0;36m(EngineCore_DP0 pid=28784)[0;0m     return layer_class(
[0;36m(EngineCore_DP0 pid=28784)[0;0m            ^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=28784)[0;0m   File "/usr/local/lib/python3.11/dist-packages/vllm/model_executor/models/nemotron_h.py", line 357, in __init__
[0;36m(EngineCore_DP0 pid=28784)[0;0m     self.mixer = NemotronHMoE(
[0;36m(EngineCore_DP0 pid=28784)[0;0m                  ^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=28784)[0;0m   File "/usr/local/lib/python3.11/dist-packages/vllm/model_executor/models/nemotron_h.py", line 190, in __init__
[0;36m(EngineCore_DP0 pid=28784)[0;0m     self.experts = SharedFusedMoE(
[0;36m(EngineCore_DP0 pid=28784)[0;0m                    ^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=28784)[0;0m   File "/usr/local/lib/python3.11/dist-packages/vllm/model_executor/layers/fused_moe/shared_fused_moe.py", line 28, in __init__
[0;36m(EngineCore_DP0 pid=28784)[0;0m     super().__init__(**kwargs)
[0;36m(EngineCore_DP0 pid=28784)[0;0m   File "/usr/local/lib/python3.11/dist-packages/vllm/model_executor/layers/fused_moe/layer.py", line 639, in __init__
[0;36m(EngineCore_DP0 pid=28784)[0;0m     self.quant_method.create_weights(layer=self, **moe_quant_params)
[0;36m(EngineCore_DP0 pid=28784)[0;0m   File "/usr/local/lib/python3.11/dist-packages/vllm/model_executor/layers/fused_moe/unquantized_fused_moe_method.py", line 129, in create_weights
[0;36m(EngineCore_DP0 pid=28784)[0;0m     torch.empty(
[0;36m(EngineCore_DP0 pid=28784)[0;0m   File "/usr/local/lib/python3.11/dist-packages/torch/utils/_device.py", line 103, in __torch_function__
[0;36m(EngineCore_DP0 pid=28784)[0;0m     return func(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=28784)[0;0m            ^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=28784)[0;0m torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 1.19 GiB. GPU 0 has a total capacity of 44.40 GiB of which 492.12 MiB is free. Process 3366432 has 43.91 GiB memory in use. Of the allocated memory 43.38 GiB is allocated by PyTorch, and 38.94 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[rank0]:[W212 18:49:44.650285413 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
[0;36m(APIServer pid=28558)[0;0m Traceback (most recent call last):
[0;36m(APIServer pid=28558)[0;0m   File "/usr/local/bin/vllm", line 8, in <module>
[0;36m(APIServer pid=28558)[0;0m     sys.exit(main())
[0;36m(APIServer pid=28558)[0;0m              ^^^^^^
[0;36m(APIServer pid=28558)[0;0m   File "/usr/local/lib/python3.11/dist-packages/vllm/entrypoints/cli/main.py", line 73, in main
[0;36m(APIServer pid=28558)[0;0m     args.dispatch_function(args)
[0;36m(APIServer pid=28558)[0;0m   File "/usr/local/lib/python3.11/dist-packages/vllm/entrypoints/cli/serve.py", line 111, in cmd
[0;36m(APIServer pid=28558)[0;0m     uvloop.run(run_server(args))
[0;36m(APIServer pid=28558)[0;0m   File "/usr/local/lib/python3.11/dist-packages/uvloop/__init__.py", line 92, in run
[0;36m(APIServer pid=28558)[0;0m     return runner.run(wrapper())
[0;36m(APIServer pid=28558)[0;0m            ^^^^^^^^^^^^^^^^^^^^^
[0;36m(APIServer pid=28558)[0;0m   File "/usr/lib/python3.11/asyncio/runners.py", line 118, in run
[0;36m(APIServer pid=28558)[0;0m     return self._loop.run_until_complete(task)
[0;36m(APIServer pid=28558)[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(APIServer pid=28558)[0;0m   File "uvloop/loop.pyx", line 1518, in uvloop.loop.Loop.run_until_complete
[0;36m(APIServer pid=28558)[0;0m   File "/usr/local/lib/python3.11/dist-packages/uvloop/__init__.py", line 48, in wrapper
[0;36m(APIServer pid=28558)[0;0m     return await main
[0;36m(APIServer pid=28558)[0;0m            ^^^^^^^^^^
[0;36m(APIServer pid=28558)[0;0m   File "/usr/local/lib/python3.11/dist-packages/vllm/entrypoints/openai/api_server.py", line 919, in run_server
[0;36m(APIServer pid=28558)[0;0m     await run_server_worker(listen_address, sock, args, **uvicorn_kwargs)
[0;36m(APIServer pid=28558)[0;0m   File "/usr/local/lib/python3.11/dist-packages/vllm/entrypoints/openai/api_server.py", line 938, in run_server_worker
[0;36m(APIServer pid=28558)[0;0m     async with build_async_engine_client(
[0;36m(APIServer pid=28558)[0;0m   File "/usr/lib/python3.11/contextlib.py", line 210, in __aenter__
[0;36m(APIServer pid=28558)[0;0m     return await anext(self.gen)
[0;36m(APIServer pid=28558)[0;0m            ^^^^^^^^^^^^^^^^^^^^^
[0;36m(APIServer pid=28558)[0;0m   File "/usr/local/lib/python3.11/dist-packages/vllm/entrypoints/openai/api_server.py", line 147, in build_async_engine_client
[0;36m(APIServer pid=28558)[0;0m     async with build_async_engine_client_from_engine_args(
[0;36m(APIServer pid=28558)[0;0m   File "/usr/lib/python3.11/contextlib.py", line 210, in __aenter__
[0;36m(APIServer pid=28558)[0;0m     return await anext(self.gen)
[0;36m(APIServer pid=28558)[0;0m            ^^^^^^^^^^^^^^^^^^^^^
[0;36m(APIServer pid=28558)[0;0m   File "/usr/local/lib/python3.11/dist-packages/vllm/entrypoints/openai/api_server.py", line 188, in build_async_engine_client_from_engine_args
[0;36m(APIServer pid=28558)[0;0m     async_llm = AsyncLLM.from_vllm_config(
[0;36m(APIServer pid=28558)[0;0m                 ^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(APIServer pid=28558)[0;0m   File "/usr/local/lib/python3.11/dist-packages/vllm/v1/engine/async_llm.py", line 228, in from_vllm_config
[0;36m(APIServer pid=28558)[0;0m     return cls(
[0;36m(APIServer pid=28558)[0;0m            ^^^^
[0;36m(APIServer pid=28558)[0;0m   File "/usr/local/lib/python3.11/dist-packages/vllm/v1/engine/async_llm.py", line 155, in __init__
[0;36m(APIServer pid=28558)[0;0m     self.engine_core = EngineCoreClient.make_async_mp_client(
[0;36m(APIServer pid=28558)[0;0m                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(APIServer pid=28558)[0;0m   File "/usr/local/lib/python3.11/dist-packages/vllm/v1/engine/core_client.py", line 122, in make_async_mp_client
[0;36m(APIServer pid=28558)[0;0m     return AsyncMPClient(*client_args)
[0;36m(APIServer pid=28558)[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(APIServer pid=28558)[0;0m   File "/usr/local/lib/python3.11/dist-packages/vllm/v1/engine/core_client.py", line 819, in __init__
[0;36m(APIServer pid=28558)[0;0m     super().__init__(
[0;36m(APIServer pid=28558)[0;0m   File "/usr/local/lib/python3.11/dist-packages/vllm/v1/engine/core_client.py", line 479, in __init__
[0;36m(APIServer pid=28558)[0;0m     with launch_core_engines(vllm_config, executor_class, log_stats) as (
[0;36m(APIServer pid=28558)[0;0m   File "/usr/lib/python3.11/contextlib.py", line 144, in __exit__
[0;36m(APIServer pid=28558)[0;0m     next(self.gen)
[0;36m(APIServer pid=28558)[0;0m   File "/usr/local/lib/python3.11/dist-packages/vllm/v1/engine/utils.py", line 933, in launch_core_engines
[0;36m(APIServer pid=28558)[0;0m     wait_for_engine_startup(
[0;36m(APIServer pid=28558)[0;0m   File "/usr/local/lib/python3.11/dist-packages/vllm/v1/engine/utils.py", line 992, in wait_for_engine_startup
[0;36m(APIServer pid=28558)[0;0m     raise RuntimeError(
[0;36m(APIServer pid=28558)[0;0m RuntimeError: Engine core initialization failed. See root cause above. Failed core proc(s): {}
