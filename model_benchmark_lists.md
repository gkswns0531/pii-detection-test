# PII ê²€ì¶œ ì†Œí˜• ëª¨ë¸ ë²¤ì¹˜ë§ˆí¬ â€” Top 10 ë¦¬ìŠ¤íŠ¸ (2026ë…„ 2ì›” ê¸°ì¤€)

## Category 1: â‰¤1B Parameters

| Rank | Model | Params | Release | Developer | Key Strengths |
|------|-------|--------|---------|-----------|---------------|
| 1 | **Qwen3-0.6B** | 0.6B | 2025.05 | Alibaba | ìµœê°• sub-1B, 119ê°œ ì–¸ì–´, thinking/no-think ë“€ì–¼ëª¨ë“œ |
| 2 | Gemma-3-1B-IT | 1.0B | 2025.03 | Google | ë©€í‹°ëª¨ë‹¬, 140+ê°œ ì–¸ì–´, 128K context |
| 3 | Llama-3.2-1B-Instruct | 1.0B | 2024.09 | Meta | ë²”ìš© NLP, íŒŒì¸íŠœë‹ íš¨ìœ¨ ìµœê³  |
| 4 | SmolLM2-1.7B-Instruct | 1.7B | 2024.11 | HuggingFace | ì™„ì „ ì˜¤í”ˆì†ŒìŠ¤, ê²½ëŸ‰ ì¶”ë¡  |
| 5 | Qwen2.5-0.5B-Instruct | 0.5B | 2024.09 | Alibaba | 128K context, 29ê°œ ì–¸ì–´ |
| 6 | Gemma-3n-E2B-IT | ~2B eff | 2025.06 | Google | ì˜¨ë””ë°”ì´ìŠ¤ ìµœì í™”, í…ìŠ¤íŠ¸+ì´ë¯¸ì§€+ì˜¤ë””ì˜¤ |
| 7 | SmolLM2-360M-Instruct | 0.36B | 2024.11 | HuggingFace | ì´ˆê²½ëŸ‰ ì—£ì§€ |
| 8 | TinyLlama-1.1B-Chat | 1.1B | 2024.01 | Community | 3Tí† í° í•™ìŠµ, ëª¨ë°”ì¼ |
| 9 | Danube3-500M | 0.5B | 2024.11 | H2O.ai | ê²½ëŸ‰ ëŒ€í™”í˜• |
| 10 | SmolLM2-135M-Instruct | 0.13B | 2024.11 | HuggingFace | ê°€ì¥ ì‘ì€ ì‹¤ìš© ëª¨ë¸ |

## Category 2: 1B < x â‰¤ 3B Parameters

| Rank | Model | Params | Release | Developer | Key Strengths |
|------|-------|--------|---------|-----------|---------------|
| 1 | **SmolLM3-3B** | 3.0B | 2025.H2 | HuggingFace | Llama-3.2-3B/Qwen2.5-3B ëŠ¥ê°€, think/no_think ë“€ì–¼ëª¨ë“œ |
| 2 | Qwen3-1.7B | 1.7B | 2025.05 | Alibaba | Qwen2.5-3B ëŠ¥ê°€, 119ê°œ ì–¸ì–´ |
| 3 | EXAONE-3.5-2.4B-Instruct | 2.4B | 2024.12 | LG AI | í•œêµ­ì–´-ì˜ì–´ ì´ì¤‘ì–¸ì–´ íŠ¹í™” |
| 4 | Ministral-3B-Instruct-2512 | 3.4B | 2025.12 | Mistral AI | ìµœì‹  ì—£ì§€ ë©€í‹°ëª¨ë‹¬ SLM |
| 5 | Llama-3.2-3B-Instruct | 3.0B | 2024.09 | Meta | ë²”ìš© í…ìŠ¤íŠ¸, íŒŒì¸íŠœë‹ ìƒíƒœê³„ |
| 6 | Qwen2.5-3B-Instruct | 3.0B | 2024.09 | Alibaba | ì•ˆì •ì  ë‹¤êµ­ì–´ ê¸°ë°˜ |
| 7 | Phi-3-mini-instruct | 3.8B | 2024.06 | Microsoft | ì¶”ë¡ Â·ìˆ˜í•™ ê°•ì  |
| 8 | StableLM-2-1.6B | 1.6B | 2024.01 | Stability AI | ìœ ëŸ½ì–´ + ë‹¤êµ­ì–´ |
| 9 | MobileLLaMA-2.7B | 2.7B | 2024.05 | Community | ëª¨ë°”ì¼ ìµœì í™”, 40% ë¹ ë¥¸ ì¶”ë¡  |
| 10 | Gemma-2-2B-IT | 2.6B | 2024.06 | Google | ì•ˆì •ì  ë‹¤êµ­ì–´ ê¸°ë°˜ |

## Category 3: 3B < x â‰¤ 10B Parameters

| Rank | Model | Params | Release | Developer | Key Strengths |
|------|-------|--------|---------|-----------|---------------|
| 1 | **Falcon-H1R-7B** | 7.0B | 2026.01 | TII | ğŸ†• ìµœì‹ ! Transformer-Mamba2 í•˜ì´ë¸Œë¦¬ë“œ, 32Bê¸‰ ì¶”ë¡ , 256K context |
| 2 | Qwen3-4B-Instruct-2507 | 4.0B | 2025.07 | Alibaba | ìµœì‹  distill, ì¼ë¶€ ë²¤ì¹˜ì—ì„œ 8B ëŠ¥ê°€ |
| 3 | EXAONE-3.5-7.8B-Instruct | 7.8B | 2024.12 | LG AI | í•œêµ­ì–´-ì˜ì–´ ì´ì¤‘ì–¸ì–´ ìµœê°• |
| 4 | Qwen3-8B | 8.0B | 2025.05 | Alibaba | Qwen2.5-14B ëŠ¥ê°€, ë“€ì–¼ëª¨ë“œ |
| 5 | Gemma-3-4B-IT | 4.0B | 2025.03 | Google | ë©€í‹°ëª¨ë‹¬, 140+ê°œ ì–¸ì–´ |
| 6 | Phi-4-mini-instruct | 3.8B | 2024.12 | Microsoft | ì¶”ë¡ Â·ë‹¤êµ­ì–´ 7-9Bê¸‰ ì„±ëŠ¥ |
| 7 | DeepSeek-R1-Distill-Qwen-7B | 7.0B | 2025.01 | DeepSeek | ìˆ˜í•™/ì½”ë”© ìµœê°• (MATH-500 92.8%) |
| 8 | Llama-3.1-8B-Instruct | 8.0B | 2024.07 | Meta | ë²”ìš© 8B ê¸°ì¤€ì„  |
| 9 | Mistral-7B-Instruct-v0.3 | 7.2B | 2024.05 | Mistral AI | ìœ ëŸ½ì–´ ê°•ì  |
| 10 | Gemma-3-12B-IT | 12.0B | 2025.03 | Google | (10B ì´ˆê³¼ì´ë‚˜ ê²½ìŸë ¥) |

---

## ì‹¤ì œ í…ŒìŠ¤íŠ¸ ì™„ë£Œ ëª¨ë¸ (ì´ 11ê°œ + ë² ì´ìŠ¤ë¼ì¸ 1ê°œ)

í‰ê°€ í™˜ê²½: NVIDIA B200, vLLM v0.15.1, 299ê°œ í•œêµ­ì–´ PII ê²€ì¶œ í…ŒìŠ¤íŠ¸ ì¼€ì´ìŠ¤

### Baseline

| Model | Params | HuggingFace ID | Acc | F1 | Median Latency | ë¹„ê³  |
|-------|--------|----------------|-----|-----|----------------|------|
| Qwen3-30B-A3B-Instruct-2507-FP8 | 30B (3B active) | `Qwen/Qwen3-30B-A3B-Instruct-2507-FP8` | 90.30% | 94.90% | - | MoE ë² ì´ìŠ¤ë¼ì¸ |

### Category 1: â‰¤1B (3ê°œ í…ŒìŠ¤íŠ¸)

| Model | Params | HuggingFace ID | Acc | F1 | ë¹„ê³  |
|-------|--------|----------------|-----|-----|------|
| Qwen3-0.6B | 0.6B | `Qwen/Qwen3-0.6B` | 13.71% | 39.86% | `--no-think` í•„ìš” |
| Gemma-3-1B-IT | 1.0B | `google/gemma-3-1b-it` | 13.38% | 39.01% | `--block-size 32` í•„ìš” |
| Llama-3.2-1B-Instruct | 1.0B | `meta-llama/Llama-3.2-1B-Instruct` | 6.02% | 16.83% | FP í­ë°œ |

### Category 2: 1B-3B (3ê°œ í…ŒìŠ¤íŠ¸ + 1ê°œ ìŠ¤í‚µ)

| Model | Params | HuggingFace ID | Acc | F1 | Median Latency | ë¹„ê³  |
|-------|--------|----------------|-----|-----|----------------|------|
| Qwen3-1.7B | 1.7B | `Qwen/Qwen3-1.7B` | 62.54% | 77.22% | 0.409s | `--no-think` í•„ìš” |
| SmolLM3-3B | 3.0B | `HuggingFaceTB/SmolLM3-3B` | 60.20% | 70.75% | 0.414s | P ë†’ê³  R ë‚®ìŒ |
| Llama-3.2-3B-Instruct | 3.0B | `meta-llama/Llama-3.2-3B-Instruct` | 16.72% | 26.44% | - | FP í­ë°œ |
| ~~EXAONE-3.5-2.4B~~ | 2.4B | `LGAI-EXAONE/EXAONE-3.5-2.4B-Instruct` | - | - | - | transformers í˜¸í™˜ ì´ìŠˆ (RopeParameters) |

### Category 3: 3-10B (4ê°œ í…ŒìŠ¤íŠ¸ + 1ê°œ ìŠ¤í‚µ)

| Model | Params | HuggingFace ID | Acc | F1 | Median Latency | ë¹„ê³  |
|-------|--------|----------------|-----|-----|----------------|------|
| Qwen3-8B | 8.0B | `Qwen/Qwen3-8B` | 79.60% | 90.04% | 0.947s | `--no-think` í•„ìš”, 30Bì— ê°€ì¥ ê·¼ì ‘ |
| Qwen3-4B-Instruct-2507 | 4.0B | `Qwen/Qwen3-4B-Instruct-2507` | 79.60% | 87.79% | 0.622s | `--no-think` í•„ìš”, ê°€ì„±ë¹„ ìµœê°• |
| Falcon-H1R-7B | 7.0B | `tiiuae/Falcon-H1R-7B` | 47.49% | 69.66% | 1.061s | Mamba2 í•˜ì´ë¸Œë¦¬ë“œ, í•œêµ­ì–´ PII ë¶€ì§„ |
| Gemma-3-4B-IT | 4.0B | `google/gemma-3-4b-it` | 9.70% | 42.85% | - | `--block-size 32` í•„ìš”, FP í­ë°œ |
| ~~EXAONE-3.5-7.8B~~ | 7.8B | `LGAI-EXAONE/EXAONE-3.5-7.8B-Instruct` | - | - | - | transformers í˜¸í™˜ ì´ìŠˆ (RopeParameters) |

### vLLM ì‹¤í–‰ ì°¸ê³ ì‚¬í•­

| ëª¨ë¸ | vLLM ì‹¤í–‰ ëª…ë ¹ |
|------|---------------|
| Qwen ê³„ì—´ | `vllm serve <model> --max-model-len 8192` + í‰ê°€ ì‹œ `--no-think` |
| Gemma ê³„ì—´ | `vllm serve <model> --max-model-len 8192 --block-size 32` |
| Falcon-H1R | `vllm serve <model> --max-model-len 8192` |
| Llama ê³„ì—´ | `vllm serve <model> --max-model-len 8192` |
| SmolLM3 | `vllm serve <model> --max-model-len 8192` |
| EXAONE ê³„ì—´ | `--trust-remote-code` í•„ìš”í•˜ë‚˜ transformers 4.57.3 í˜¸í™˜ ë¶ˆê°€ |
